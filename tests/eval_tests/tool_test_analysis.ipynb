{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluating Tool Calling Limitations and Performance of Small LLMs\n",
    "Goal: The primary goal of this experiment is to evaluate the tool calling limitations of small LLMs (1B - 3B parameters) and to identify methods (e.g., prompting, tool descriptions) to enhance their performance.\n",
    "\n",
    "Evaluation Set: This analysis uses a custom evaluation set comprising 600 queries. The queries were all generated by Gemini, there are 15 queries per function tool and a total of 40 function tools."
   ],
   "id": "f02eceeb0d3f3235"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This initial experiment demonstrates that `llama3.2:3B` exhibits a complete degradation in accuracy when provided with more than 32 tools. The `TOOL_LIMIT` is set to 32 because any increase beyond this number results in a complete loss of tool-calling accuracy, which was a surprising outcome given that one might intuitively expect only a decrease in performance.",
   "id": "21863d62bc6413e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import utils\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.client_tool import ClientTool\n",
    "from tools import tools, tools_only_params, tools_no_extra_tags, tools_bad_function_names\n",
    "from tests import load_queries, run_client_tool_test\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "81dd26b44d0990c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up reused logger and client\n",
    "logger = utils.setup_logger()\n",
    "\n",
    "base_url = os.getenv('REMOTE_BASE_URL')\n",
    "if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        exit(1)\n",
    "\n",
    "llama_client = LlamaStackClient(base_url=base_url)"
   ],
   "id": "d0924e0cc8b66998",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "normal_client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries.json\")\n",
    "bad_function_names_client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries_bad_functions.json\")"
   ],
   "id": "1bfd68fc2436b90b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_test(models, tool_module, client_tool_queries, analysis_plot_path):\n",
    "    tool_list = []\n",
    "    for name in sorted(dir(tool_module)):\n",
    "        if len(tool_list) >= TOOL_LIMIT:\n",
    "            break\n",
    "        attribute = getattr(tool_module, name)\n",
    "        if isinstance(attribute, ClientTool):\n",
    "            tool_list.append(attribute)\n",
    "    tool_name_set = {tool.__name__ for tool in tool_list}\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        queries = load_queries(client_tool_queries)\n",
    "        if not queries:\n",
    "            logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "            continue\n",
    "\n",
    "        for query_obj in queries:\n",
    "            if query_obj[\"tool_call\"] not in tool_name_set:\n",
    "                continue\n",
    "            total_tests += 1\n",
    "            success = run_client_tool_test(model, query_obj, tool_list, llama_client, logger)\n",
    "            if success:\n",
    "                successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots(analysis_plot_path)"
   ],
   "id": "3330422654d2078e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set tool limit to max for llama3.2:3B\n",
    "TOOL_LIMIT = 32"
   ],
   "id": "3d96bc651a4e8aeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_test(\n",
    "    models=[\"meta-llama/Llama-3.2-3B-Instruct\"],\n",
    "    tool_module=tools_no_extra_tags,\n",
    "    client_tool_queries=normal_client_tool_queries,\n",
    "    analysis_plot_path=\"./results/no_extra_tools_client_tool_metrics.csv\"\n",
    ")"
   ],
   "id": "32cc19b7d46ad666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![no_extra_tags_tool_call_match_per_function_tool.jpg](results/plots/no_extra_tags_tool_call_match_per_function_tool.jpg)\n",
    "\n",
    "Based off the results, the llama3.2:3B model has quite high accuracy by just giving a good function name and `:param:` in the docstring"
   ],
   "id": "4f7fe564688ff04f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The next cell will show how adding a single tool after 32 will lead to complete accuracy loss",
   "id": "45687c818fed8fb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set tool limit to one more than the max (32)\n",
    "TOOL_LIMIT = 33"
   ],
   "id": "a3b82f4ea17308dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_test(\n",
    "    models=[\"meta-llama/Llama-3.2-3B-Instruct\"],\n",
    "    tool_module=tools_no_extra_tags,\n",
    "    client_tool_queries=normal_client_tool_queries,\n",
    "    analysis_plot_path=\"./results/no_extra_tools_client_tool_metrics.csv\"\n",
    ")"
   ],
   "id": "dbd41e175d20bd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![tool_calling_match_per_function_33_tools_3B](results/plots/tool_calling_match_per_function_33_tools_3B.jpg)\n",
    "This shows how adding a single tool after 32 completely degrades the accuracy of successful tool calls to almost 0."
   ],
   "id": "2d097a7b9b888178"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This next cell will test how adding explicit `:description:` and `:use_case:` annotations can help in increasing accuracy. It is **not** a fact that adding them will increase accuracy but for our query set it helped.",
   "id": "ccc06f630075723e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set tool limit back to max for llama3.2:3B\n",
    "TOOL_LIMIT = 32"
   ],
   "id": "112ae1854ff85184",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_test(\n",
    "    models=[\"meta-llama/Llama-3.2-3B-Instruct\"],\n",
    "    tool_module=tools,\n",
    "    client_tool_queries=normal_client_tool_queries,\n",
    "    analysis_plot_path=\"./results/normal_client_tool_metrics.csv\"\n",
    ")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![Tool Call Match Per Function Tool](results/plots/normal_tool_call_match_per_function_tool.png)\n",
    "Overall majority of the tools have 100% accuracy, and there is a slight increase in correct tool call compared to having just `:param:` annotation."
   ],
   "id": "65757d655859540f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The next few cells run experiments to test what truly matters when defining a tool: the tool name, the description, or the format of the docstring. A quick overview of how `llama-stack` parses the function when tagged with the `client_tool` decorator.\n",
    "\n",
    "```python\n",
    "def client_tool(func: T) -> ClientTool:\n",
    "    \"\"\"\n",
    "    Decorator to convert a function into a ClientTool.\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "    class _WrappedTool(ClientTool):\n",
    "        __name__ = func.__name__\n",
    "        __doc__ = func.__doc__\n",
    "        __module__ = func.__module__\n",
    "\n",
    "        def get_name(self) -> str:\n",
    "            ...\n",
    "\n",
    "        def get_description(self) -> str:\n",
    "            ...\n",
    "\n",
    "        def get_params_definition(self) -> Dict[str, Parameter]:\n",
    "            hints = get_type_hints(func)\n",
    "            # Remove return annotation if present\n",
    "            hints.pop(\"return\", None)\n",
    "\n",
    "            # Get parameter descriptions from docstring\n",
    "            params = {}\n",
    "            sig = inspect.signature(func)\n",
    "            doc = inspect.getdoc(func) or \"\"\n",
    "\n",
    "            for name, type_hint in hints.items():\n",
    "                # Look for :param name: in docstring\n",
    "                param_doc = \"\"\n",
    "                for line in doc.split(\"\\n\"):\n",
    "                    if line.strip().startswith(f\":param {name}:\"):\n",
    "                        param_doc = line.split(\":\", 2)[2].strip()\n",
    "                        break\n",
    "\n",
    "                if param_doc == \"\":\n",
    "                    raise ValueError(f\"No parameter description found for parameter {name}\")\n",
    "\n",
    "                ...\n",
    "\n",
    "            return params\n",
    "```\n",
    "Full implementation can be found [here](https://github.com/meta-llama/llama-stack-client-python/blob/645d2195c5af1c6f903cb93c293319d8f94c36cc/src/llama_stack_client/lib/agents/client_tool.py#L150-L170).\n",
    "An important thing to realize is that `llama-stack` **purposefully** disregards the return information from the docstring. Also that the docstring only **requires** one annotation, `:param:`, and everything _above_ that will be parsed together."
   ],
   "id": "765e19f8bba53e6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This next cell will test whether explicitly having `:description:` and `:use_case:` annotations help, compared to including them without any annotation.\n",
    "\n",
    "Ex.\n",
    "```python\n",
    "@client_tool\n",
    "def add_two_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    :description: Adds two numbers.\n",
    "    :use_case: Use when the user wants to find the sum, total, or combined value of two numbers.\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    :returns: The sum of `a` and `b`.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```\n",
    "\n",
    "compared to\n",
    "\n",
    "```python\n",
    "@client_tool\n",
    "def add_two_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Adds two numbers.\n",
    "    Use when the user wants to find the sum, total, or combined value of two numbers.\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    :returns: The sum of `a` and `b`.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```"
   ],
   "id": "466e2edfe4fdf8fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This next cell will test whether the tool name matters at all. To do this test, all functions were renamed to `function_1`, `function_2`, etc. but the docstring was left unchanged.\n",
    "\n",
    "Ex.\n",
    "```python\n",
    "@client_tool\n",
    "def function_1(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    :description: Adds two numbers.\n",
    "    :use_case: Use when the user wants to find the sum, total, or combined value of two numbers.\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    :returns: The sum of `a` and `b`.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```"
   ],
   "id": "ee5f00dc9a51ee1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_test(\n",
    "    models=[\"meta-llama/Llama-3.2-3B-Instruct\"],\n",
    "    tool_module=tools_bad_function_names,\n",
    "    client_tool_queries=bad_function_names_client_tool_queries,\n",
    "    analysis_plot_path=\"./results/bad_function_names_client_tool_metrics.csv\"\n",
    ")"
   ],
   "id": "afd3207af6490e75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "![bad_function_names_tool_call_match_per_function_tool](results/plots/bad_function_names_tool_call_match_per_function_tool.jpg)\n",
    "\n",
    "The results show a sharp degrade in accuracy, emphasizing the importance of good function naming practices. Another experiment which could spawn from this is seeing whether using unit test style function naming for client tools and MCP servers works well."
   ],
   "id": "20eb271445aee556"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This next cell will test whether the tool description matters at all. To do this test, all docstrings have been reduced to only contain the required `:param:` annotation and function names have been kept the same.\n",
    "\n",
    "Ex.\n",
    "```python\n",
    "@client_tool\n",
    "def add_two_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```"
   ],
   "id": "7e73ab1372d7ba89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_test(\n",
    "    models=[\"meta-llama/Llama-3.2-3B-Instruct\"],\n",
    "    tool_module=tools_only_params,\n",
    "    client_tool_queries=normal_client_tool_queries,\n",
    "    analysis_plot_path=\"./results/only_params_client_tool_metrics.csv\"\n",
    ")"
   ],
   "id": "84ce35871570aa05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![only_params_tool_call_match_per_function.jpg](results/plots/only_params_tool_call_match_per_function.jpg)\n",
    "\n",
    "The results show that removing all details from the docstring other than the required `:param:` annotation does not lead to large decrease in accuracy. This is likely why `llama-stack` only requires the `:param:` annotation but nothing else, like `:use_case:`."
   ],
   "id": "bee5ffa29c5b3e20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## We will now run the same evaluation set on the well constructed tools but swap `llama3.2:3B` with `llama3.2:1B`.",
   "id": "ec74daaf4026b94f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_test(\n",
    "    models=[\"llama3.2:1b\"],\n",
    "    tool_module=tools,\n",
    "    client_tool_queries=normal_client_tool_queries,\n",
    "    analysis_plot_path=\"./results/normal_client_tool_metrics_1B.csv\"\n",
    ")"
   ],
   "id": "ca60c1aea5b97699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "![tool_calling_match_per_function_23_tools_1B.png.jpg](results/plots/tool_calling_match_per_function_23_tools_1B.png.jpg)\n",
    "\n",
    "The results show that llama3.2:1B is far worse at tool calling compared to llama3.2:3B when using all the best practices we used above."
   ],
   "id": "ba8bab8a04bd6532"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "The following observations have been made using our eval set of 600 queries.\n",
    "- `llama3.2:3B` can handle a maximum of 32 tools before a complete degradation in accuracy.\n",
    "- Well named functions are more important than a well written function description.\n",
    "- Explicitly added `:description:` and `:use_case:` showed a slight improve in accuracy for our eval set.\n",
    "- `llama3.2:1B` is too small of a model and is extremely inconsistent at tool calling.\n",
    "  - Important note is that the quantized model was used which could have been a big factor for low performance"
   ],
   "id": "b07170f3bdc48090"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
