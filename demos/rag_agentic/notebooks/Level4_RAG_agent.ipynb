{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
      "metadata": {},
      "source": [
        "# Level 4: RAG Agent\n",
        "\n",
        "This notebook presents an example of executing queries with a RAG agent in Llama Stack. It shows how to initialize an agent with the RAG tool provided by Llama Stack and to invoke it such that retrieval from a vector DB is activated only when necessary. The notebook also covers document ingestion using the RAG tool.\n",
        "\n",
        "For a simple (non-agentic) RAG tutorial, please refer to [Level1_simple_RAG.ipynb](./Level1_simple_RAG.ipynb).\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook covers the following steps:\n",
        "\n",
        "1. Connecting to a llama stack server.\n",
        "2. Indexing a collection of documents into a vector DB for later retrieval.\n",
        "3. Initializing the agent capable of retrieving content from vector DB via tool use.\n",
        "4. Launching the agent and using it to answer user queries about the documents.\n",
        "\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, ensure you have the following:\n",
        "- Followed the instructions in the [Setup Guide](./Level0_getting_started_with_Llama_Stack.ipynb) notebook. \n",
        "- Llama Stack server should be using milvus as its vector DB provider.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
      "metadata": {},
      "source": [
        "## 1. Setting Up this Notebook\n",
        "We will start with a few imports needed for this demo only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "854e7cb4-aed9-4098-adc1-a66f4c9e6ce3",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "from llama_stack_client import Agent, RAGDocument\n",
        "from llama_stack_client.lib.agents.event_logger import EventLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ab4244-e7af-405b-b0c3-4bf00411f26e",
      "metadata": {},
      "source": [
        "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b87b139-bd18-47b2-889a-1b8ed3018655",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for accessing the environment variables\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# for communication with Llama Stack\n",
        "from llama_stack_client import LlamaStackClient\n",
        "\n",
        "# pretty print of the results returned from the model/agent\n",
        "import sys\n",
        "sys.path.append('..')  \n",
        "from src.utils import step_printer\n",
        "from termcolor import cprint\n",
        "\n",
        "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
        "\n",
        "\n",
        "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
        "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
        "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
        "if tavily_search_api_key is None:\n",
        "    provider_data = None\n",
        "else:\n",
        "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
        "\n",
        "\n",
        "client = LlamaStackClient(\n",
        "    base_url=base_url,\n",
        "    provider_data=provider_data\n",
        ")\n",
        "    \n",
        "print(f\"Connected to Llama Stack server\")\n",
        "\n",
        "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
        "model_id = \"granite32-8b\"\n",
        "\n",
        "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
        "if temperature > 0.0:\n",
        "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
        "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
        "else:\n",
        "    strategy = {\"type\": \"greedy\"}\n",
        "\n",
        "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
        "\n",
        "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
        "sampling_params = {\n",
        "    \"strategy\": strategy,\n",
        "    \"max_tokens\": max_tokens,\n",
        "}\n",
        "\n",
        "stream_env = os.getenv(\"STREAM\", \"False\")\n",
        "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
        "# any value non equal to 'False' will be considered as 'True'\n",
        "stream = (stream_env != \"False\")\n",
        "\n",
        "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357655b5-cade-46f4-9f57-be5dcef9abc2",
      "metadata": {},
      "source": [
        "Finally, we will create a unique name for the document collection to be used for RAG ingestion and retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583421f3-5c77-4964-b525-12f967c20816",
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9203de51-f570-44ab-8130-36333a54888b",
      "metadata": {},
      "source": [
        "## 2. Indexing the Documents\n",
        "- Initialize a new document collection in the target vector DB. All parameters related to the vector DB, such as the embedding model and dimension, must be specified here.\n",
        "- Provide a list of document URLs to the RAG tool. Llama Stack will handle the fetching, conversion and chunking of the documents' content automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd4664a-ff7f-4474-b6af-3a4ad3f73052",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# define and register the document collection to be used\n",
        "client.vector_dbs.register(\n",
        "    vector_db_id=vector_db_id,\n",
        "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
        "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
        "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
        ")\n",
        "\n",
        "# ingest the documents into the newly created document collection\n",
        "urls = [\n",
        "    (\"https://www.openshift.guide/openshift-guide-screen.pdf\", \"application/pdf\"),\n",
        "]\n",
        "documents = [\n",
        "    RAGDocument(\n",
        "        document_id=f\"num-{i}\",\n",
        "        content=url,\n",
        "        mime_type=url_type,\n",
        "        metadata={},\n",
        "    )\n",
        "    for i, (url, url_type) in enumerate(urls)\n",
        "]\n",
        "client.tool_runtime.rag_tool.insert(\n",
        "    documents=documents,\n",
        "    vector_db_id=vector_db_id,\n",
        "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
      "metadata": {},
      "source": [
        "## 3. Executing queries via the RAG agent\n",
        "- Initialize an agent with a list of tools including the built-in RAG tool. The RAG tool specification must include a list of document collection IDs to retrieve from.\n",
        "- For each prompt, initialize a new agent session, execute a turn during which a retrieval call may be requested, and output the reply received from the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b9baa2-4739-426a-b79a-2ff90f44c023",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"How to install OpenShift?\",\n",
        "]\n",
        "\n",
        "# initializing the agent\n",
        "agent = Agent(\n",
        "    client,\n",
        "    model=model_id,\n",
        "    instructions=\"You are a helpful assistant. You must use the knowledge search tool to answer user questions.\",\n",
        "    sampling_params=sampling_params,\n",
        "    # we make our agent aware of the RAG tool by including builtin::rag/knowledge_search in the list of tools\n",
        "    tools=[\n",
        "        dict(\n",
        "            name=\"builtin::rag\",\n",
        "            args={\n",
        "                \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
        "            },\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "\n",
        "for prompt in queries:\n",
        "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
        "    \n",
        "    # create a new turn with a new session ID for each prompt\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
        "        stream=stream,\n",
        "    )\n",
        "    \n",
        "    # print the response, including tool calls output\n",
        "    if stream:\n",
        "        for log in EventLogger().log(response):\n",
        "            log.print()\n",
        "    else:\n",
        "        step_printer(response.steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "This notebook demonstrated how to implement a RAG agent with Llama Stack. We did this by creating an agent and giving it access to the builtin RAG tool, then invoking the agent on the specified query.\n",
        "\n",
        "Now let's move on to discuss another type of tool we can use to further enhance our models capabilities, MCP Servers, in the next notebook: [Agents and MCP Servers](./Level5_agents_and_mcp.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc0fc00",
      "metadata": {},
      "source": [
        "#### Any Feedback?\n",
        "\n",
        "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
