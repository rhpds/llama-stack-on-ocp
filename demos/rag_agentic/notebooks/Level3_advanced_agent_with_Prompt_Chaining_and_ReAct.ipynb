{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 3: Advanced Agent Capabilities with Prompt Chaining and ReAct Agent\n",
        "\n",
        "Building on the simple agent introduced in [Level 2](Level2_simple_agent_with_websearch.ipynb), this tutorial continues the agent-focused section of our series by introducing techniques that make the agent smarter and more autonomous: **Prompt Chaining** and the **ReAct (Reasoning + Acting) framework**. These approaches allow the agent to complete multi-step tasks, dynamically choose tools, and adjust its behavior based on context.\n",
        "\n",
        "- **Prompt Chaining** connects multiple prompts into a coherent sequence, allowing the agent to maintain context and perform multi-step reasoning across tool invocations. \n",
        "- **ReAct Agent** combines reasoning and acting steps in a loop, enabling the agent to make decisions, use tools dynamically, and adapt based on intermediate results. \n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook, you'll explore three agent configurations:\n",
        "1. **Simple Agent (Baseline)** – Uses a single web search tool.\n",
        "2. **Prompt Chaining** – Performs structured, multi-step reasoning by chaining prompts and responses.\n",
        "3. **ReAct Agent** – Dynamically plans and executes actions using a loop of reasoning and tool use.\n",
        "\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting this notebook, ensure that you have:\n",
        "- Followed the instructions in the [Setup Guide](./Level0_getting_started_with_Llama_Stack.ipynb) notebook. \n",
        "- A Tavily API key. It is critical for this notebook to run correctly. You can register for one at [https://tavily.com/](https://tavily.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setting Up this Notebook\n",
        "We will start with a few imports needed for this demo only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_stack_client import Agent\n",
        "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
        "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
        "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
        "import sys\n",
        "sys.path.append('..') \n",
        "from src.client_tools import get_location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for accessing the environment variables\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# for communication with Llama Stack\n",
        "from llama_stack_client import LlamaStackClient\n",
        "\n",
        "# pretty print of the results returned from the model/agent\n",
        "import sys\n",
        "sys.path.append('..')  \n",
        "from src.utils import step_printer\n",
        "from termcolor import cprint\n",
        "\n",
        "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
        "\n",
        "\n",
        "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
        "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
        "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
        "if tavily_search_api_key is None:\n",
        "    provider_data = None\n",
        "else:\n",
        "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
        "\n",
        "\n",
        "client = LlamaStackClient(\n",
        "    base_url=base_url,\n",
        "    provider_data=provider_data\n",
        ")\n",
        "    \n",
        "print(f\"Connected to Llama Stack server\")\n",
        "\n",
        "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
        "model_id = \"granite32-8b\"\n",
        "\n",
        "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
        "if temperature > 0.0:\n",
        "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
        "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
        "else:\n",
        "    strategy = {\"type\": \"greedy\"}\n",
        "\n",
        "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
        "\n",
        "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
        "sampling_params = {\n",
        "    \"strategy\": strategy,\n",
        "    \"max_tokens\": max_tokens,\n",
        "}\n",
        "\n",
        "stream_env = os.getenv(\"STREAM\", \"False\")\n",
        "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
        "# any value non equal to 'False' will be considered as 'True'\n",
        "stream = (stream_env != \"False\")\n",
        "\n",
        "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Simple Agent (Baseline)\n",
        "Same agent setup as [Level 2 notebook](Level2_simple_agent_with_websearch.ipynb). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = Agent(\n",
        "    client, \n",
        "    model=model_id,\n",
        "    instructions=\"\"\"You are a helpful websearch assistant. When you are asked to search the latest you must use a tool. \n",
        "            Whenever a tool is called, be sure return the response in a friendly and helpful tone.\n",
        "            \"\"\" ,\n",
        "    tools=[\"builtin::websearch\"],\n",
        "    sampling_params=sampling_params\n",
        ")\n",
        "user_prompts = [\n",
        "    \"Are there any immediate weather-related risks in my area that could disrupt network connectivity or system availability?\",\n",
        "]\n",
        "for prompt in user_prompts:\n",
        "    print(\"\\n\"+\"=\"*50)\n",
        "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
        "    print(\"=\"*50)\n",
        "    session_id = agent.create_session(\"web-session\")\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=stream\n",
        "    )\n",
        "    if stream:\n",
        "        for log in EventLogger().log(response):\n",
        "            log.print()\n",
        "    else:\n",
        "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output Analysis\n",
        "\n",
        "In this example, since the agent is unaware of the users location, it hallucinates one and generates an incorrect search query. This misidentification leads to inaccurate information about potential weather-related risks.\n",
        "\n",
        "This is where Prompt Chaining comes in. Prompt chaining allows the agent to:\n",
        "1. Maintain context across multiple queries\n",
        "2. Chain multiple tools together\n",
        "3. Use previous interactions to inform current decisions\n",
        "\n",
        "Let’s see how prompt chaining can improve the accuracy of the response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prompt chaining with websearch tool and client tool\n",
        "\n",
        "In this section, we demonstrate a more sophisticated use case that combines the use of two tools: location detection and web search.\n",
        "\n",
        "1. **Automatic Location Detection**: Use the `get_location` client tool to automatically determine the user's current location.\n",
        "2. **Contextual Search**: Leverage the detected location to formulate the correct websearch query.\n",
        "\n",
        "For example, when a user asks \"Are there any weather-related risks in my area that could disrupt network connectivity or system availability?\", the agent will:\n",
        "- First detect the user's current location using `get_location`.\n",
        "- Then use that location to search for nearby weather-related risks.\n",
        "- Finally, present a comprehensive response.\n",
        "\n",
        "This demonstrates how the builtin websearch tool and custom client tools can work together to provide intelligent, context-aware responses without requiring explicit location input from the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = Agent(\n",
        "    client, \n",
        "    model=model_id,\n",
        "    instructions=\"\"\"You are a helpful assistant. \n",
        "    When a user asks about their location, you MUST use the get_location tool. When you are asked to search the latest news, you MUST use the websearch tool.\n",
        "    \"\"\" ,\n",
        "    tools=[get_location, \"builtin::websearch\"],\n",
        "    sampling_params=sampling_params\n",
        ")\n",
        "user_prompts = [\n",
        "    \"Where am I?\",\n",
        "    \"Are there any immediate weather-related risks in my area that could disrupt network connectivity or system availability?\"\n",
        "]\n",
        "session_id = agent.create_session(\"prompt-chaining-session\")  # for prompt chaining, queries must share the same session_id.\n",
        "for prompt in user_prompts:\n",
        "    print(\"\\n\"+\"=\"*50)\n",
        "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
        "    print(\"=\"*50)\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=stream\n",
        "    )\n",
        "\n",
        "    if stream:\n",
        "        for log in EventLogger().log(response):\n",
        "            log.print()\n",
        "    else:\n",
        "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ReAct Agent with websearch tool and client tool\n",
        "\n",
        "This section demonstrates the ReAct (Reasoning and Acting) framework in action.\n",
        "\n",
        "Here is a walkthrough of how the ReAct agent will tackle this same \"weather near me\" problem:\n",
        "\n",
        "When asked \"Are there any weather-related risks in my area that could disrupt network connectivity or system availability?\", the agent will:\n",
        "\n",
        "1. **Reason** that it needs to get location information first.\n",
        "2. **Act** by calling the `get_location` client tool.\n",
        "3. **Observe** the location result.\n",
        "4. **Reason** that it now needs to search for weather in that location.\n",
        "5. **Act** by calling the `websearch` tool with observed location.\n",
        "6. **Observe** and processes the search results into a final answer. \n",
        "\n",
        "Unlike prompt chaining which follows fixed steps, ReAct dynamically breaks down tasks and adapts its approach based on the results of each step. This makes it more flexible and capable of handling complex, real-world queries effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = ReActAgent(\n",
        "            client=client,\n",
        "            model=model_id,\n",
        "            tools=[get_location, \"builtin::websearch\"],\n",
        "            response_format={\n",
        "                \"type\": \"json_schema\",\n",
        "                \"json_schema\": ReActOutput.model_json_schema(),\n",
        "            },\n",
        "            sampling_params=sampling_params,\n",
        "        )\n",
        "user_prompts = [\n",
        "    \"Are there any immediate weather-related risks in my area that could disrupt network connectivity or system availability?\"\n",
        "]\n",
        "session_id = agent.create_session(\"web-session\")\n",
        "for prompt in user_prompts:\n",
        "    print(\"\\n\"+\"=\"*50)\n",
        "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
        "    print(\"=\"*50)\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=stream\n",
        "    )\n",
        "    if stream:\n",
        "        for log in EventLogger().log(response):\n",
        "            log.print()\n",
        "    else:\n",
        "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "- This notebook demonstrated how to build more capable agents using Prompt Chaining and the ReAct framework.\n",
        "- It showed how agents can maintain context across multiple steps and perform structured, multi-step reasoning.\n",
        "- It highlights how ReAct enables dynamic tool selection and adaptive decision-making based on intermediate results.\n",
        "- These techniques enhance agent autonomy and make them more suitable for complex operational tasks.\n",
        "\n",
        "For further extensions, continue exploring in the next notebook: [RAG Agents](Level4_RAG_agent.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Any Feedback?\n",
        "\n",
        "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "stackclientv26",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
