{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b5ecd807-fb1c-41eb-a948-365e57396d90",
      "metadata": {},
      "source": [
        "# Level 5: Agents & MCP Tools\n",
        "\n",
        "This notebook is for developers who are already familiar with [basic agent workflows](Level2_simple_agent_with_websearch.ipynb). \n",
        "Here, we will highlight more advanced use cases for agents where a single tool call is insufficient to complete the required task.\n",
        "\n",
        "We will also use [MCP tools](https://github.com/modelcontextprotocol/servers) (which can be deployed onto an OpenShift cluster) throughout this demo to show users how to extend their agents beyond Llama Stacks's current builtin tools and connect to many different services and data sources to build their own custom agents.  \n",
        "\n",
        "### Agent Example:\n",
        "\n",
        "This notebook will walkthrough how to build a system that can answer the following question via an agent built with Llama Stack:\n",
        "\n",
        "- *\"Review OpenShift logs for pod-123 and pod-456. Categorize each as either ‘Normal’ or ‘Error’. If any are considered to be ‘Error’, send a Slack message to the ops team. Otherwise, show a simple summary.\"*\n",
        "\n",
        "\n",
        "### MCP Tools:\n",
        "\n",
        "#### OpenShift MCP Server\n",
        "\n",
        "Throughout this notebook we will be relying on the [kubernetes-mcp-server](https://github.com/manusa/kubernetes-mcp-server) by [manusa](https://github.com/manusa) to interact with our OpenShift cluster. Please see installation instructions below if you do not already have this deployed in your environment. \n",
        "\n",
        "* [OpenShift MCP installation instructions](../../../kubernetes/mcp-servers/openshift-mcp/README.md)\n",
        "\n",
        "#### Slack MCP Server\n",
        "We will also be using the [Slack MCP Server](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/slack) in this notebook. Please see installation instructions below if you do not already have this deployed in your environment. \n",
        "\n",
        "* [Slack MCP installation instructions](../../../kubernetes/mcp-servers/slack-mcp/README.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a77a6454",
      "metadata": {},
      "source": [
        "## Pre-Requisites\n",
        "\n",
        "Before starting this notebook, ensure that you have:\n",
        "- Followed the instructions in the [Setup Guide](./Level0_getting_started_with_Llama_Stack.ipynb) notebook.\n",
        "- Access to an OpeShift cluster with a deployment of the [OpenShift MCP server](https://github.com/opendatahub-io/llama-stack-demos/tree/main/kubernetes/mcp-servers/openshift-mcp) (see the [deployment manifests](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/kubernetes/mcp-servers/openshift-mcp) for assistance with this).\n",
        "- A Tavily API key is required. You can register for one at https://app.tavily.com/home.\n",
        "\n",
        "## Setting Up this Notebook\n",
        "We will initialize our environment as described in detail in our [\"Getting Started\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bafea86a-fcb4-4e69-8a73-1839b536b54d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for accessing the environment variables\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# for communication with Llama Stack\n",
        "from llama_stack_client import LlamaStackClient\n",
        "from llama_stack_client import Agent\n",
        "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
        "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
        "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
        "\n",
        "# pretty print of the results returned from the model/agent\n",
        "from termcolor import cprint\n",
        "import sys\n",
        "sys.path.append('..')  \n",
        "from src.utils import step_printer\n",
        "\n",
        "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
        "\n",
        "\n",
        "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
        "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
        "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
        "if tavily_search_api_key is None:\n",
        "    provider_data = None\n",
        "else:\n",
        "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
        "\n",
        "\n",
        "client = LlamaStackClient(\n",
        "    base_url=base_url,\n",
        "    provider_data=provider_data\n",
        ")\n",
        "\n",
        "print(f\"Connected to Llama Stack server\")\n",
        "\n",
        "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
        "model_id = \"granite32-8b\"\n",
        "\n",
        "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
        "if temperature > 0.0:\n",
        "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
        "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
        "else:\n",
        "    strategy = {\"type\": \"greedy\"}\n",
        "\n",
        "max_tokens = 512\n",
        "\n",
        "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
        "sampling_params = {\n",
        "    \"strategy\": strategy,\n",
        "    \"max_tokens\": max_tokens,\n",
        "}\n",
        "\n",
        "stream_env = os.getenv(\"STREAM\", \"False\")\n",
        "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
        "# any value non equal to 'False' will be considered as 'True'\n",
        "stream = (stream_env != \"False\")\n",
        "\n",
        "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66044170",
      "metadata": {},
      "source": [
        "## Validate tools are available in our Llama Stack instance\n",
        "\n",
        "When an instance of Llama Stack is redeployed, it may be the case that the tools will need to be re-registered. Also if a tool is already registered with a Llama Stack instance, trying to register another one with the same `toolgroup_id` will throw you an error.\n",
        "\n",
        "For this reason, it is recommended to validate your tools and toolgroups. The following code will check that both the `builtin::websearch`, `mcp::openshift`  and `mcp::slack` tools are correctly registered, and if not it will attempt to register them using their specific endpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2cedaf-522b-4251-886a-d8aa7b9fcd18",
      "metadata": {},
      "outputs": [],
      "source": [
        "ocp_mcp_url = os.getenv(\"REMOTE_OCP_MCP_URL\") \n",
        "slack_mcp_url = os.getenv(\"REMOTE_SLACK_MCP_URL\")\n",
        "\n",
        "registered_tools = client.tools.list()\n",
        "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
        "if \"mcp::openshift\" not in registered_toolgroups:\n",
        "    client.toolgroups.register(\n",
        "        toolgroup_id=\"mcp::openshift\",\n",
        "        provider_id=\"model-context-protocol\",\n",
        "        mcp_endpoint={\"uri\":ocp_mcp_url},\n",
        "    )\n",
        "\n",
        "if \"mcp::slack\" not in registered_toolgroups:\n",
        "    client.toolgroups.register(\n",
        "        toolgroup_id=\"mcp::slack\",\n",
        "        provider_id=\"model-context-protocol\",\n",
        "        mcp_endpoint={\"uri\":slack_mcp_url},\n",
        "   )\n",
        "print(f\"Your Llama Stack server is registered with the following tool groups @ {set(registered_toolgroups)} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa880bbc-bf69-4777-9417-ef7b13d51785",
      "metadata": {},
      "source": [
        "## Defining our Agent - Prompt Chaining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "757508bc-a3b8-493e-b003-6d9840597ab4",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_prompt= \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
        "Whenever a tool is called, be sure to return the Response in a friendly and helpful tone.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b32f2bb-d003-4347-b545-ad95da1dacff",
      "metadata": {},
      "source": [
        "### Deploy a pod with simulated error logs\n",
        "\n",
        "For the purpose of testing and retrieving logs from a pod exhibiting errors, we will deploy a pod on an OpenShift cluster that produces simulated error logs. We have a pre-built container image available for this \"fake\" error pod that you can use. With the help of the agent and the OpenShift MCP server we can deploy the pod as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd0e66d9-05b3-432e-b416-f63001b08704",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create simple agent with tools\n",
        "agent = Agent(\n",
        "    client,\n",
        "    model= model_id,  # replace this with model_id to get the value of INFERENCE_MODEL_ID environment variable\n",
        "    instructions = model_prompt , # update system prompt based on the model you are using\n",
        "    tools=[\"mcp::openshift\"],\n",
        "    tool_config={\"tool_choice\":\"auto\"},\n",
        "    sampling_params=sampling_params\n",
        ")\n",
        "\n",
        "user_prompts = [\"Create a pod called slack-test in the llama-serve namespace using the quay.io/redhat-et/failing-test-pod:latest image\"]\n",
        "session_id = agent.create_session(session_name=\"OCP_Slack_demo\")\n",
        "\n",
        "for i, prompt in enumerate(user_prompts):\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\":\"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=stream,\n",
        "    )\n",
        "    if stream:\n",
        "        for log in EventLogger().log(response):\n",
        "            log.print()\n",
        "    else:\n",
        "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdfa2fb3-3d7c-40b0-aa6a-99a8ab8217d4",
      "metadata": {},
      "source": [
        "You should see a pod `slack-test` successfully deployed in your namespace on the OpenShift cluster. If you view the logs of the pod, you should see the simulated error message as follows:\n",
        "```\n",
        "Starting container...\n",
        "Failure: Unknown Error\n",
        "Error details: Container failed due to an unexpected issue during startup.\n",
        "Potential cause: Missing dependencies, configuration errors, or permission issues.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f59216-5a02-42ed-bb4c-b062136206da",
      "metadata": {},
      "source": [
        "### Retrieve logs for erroneous pods running on OpenShift and send a message to Slack\n",
        "\n",
        "Now that we have a simulated erroneous pod running on the OpenShift cluster, we can task the agent with summarizing the logs and sending a message to Slack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc7957e4-581c-4c3d-aee7-b8e3d9f2d0c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create simple agent with tools\n",
        "agent = Agent(\n",
        "    client,\n",
        "    model= model_id,  # replace this with model_id to get the value of INFERENCE_MODEL_ID environment variable\n",
        "    instructions = model_prompt , # update system prompt based on the model you are using\n",
        "    tools=[\"mcp::openshift\", \"mcp::slack\"],\n",
        "    tool_config={\"tool_choice\":\"auto\"},\n",
        "    sampling_params=sampling_params\n",
        ")\n",
        "\n",
        "user_prompts = [\"View the logs for pod slack-test in the llama-serve OpenShift namespace. Categorize it as normal or error.\",\n",
        "               \"Summarize the results with the pod name, category along with a briefly explanation as to why you categorized it as normal or error. Respond with plain text only. Do not wrap your response in additional quotation marks.\",\n",
        "               \"Send a message with the summarization to the demos channel on Slack.\"]\n",
        "session_id = agent.create_session(session_name=\"OCP_Slack_demo\")\n",
        "\n",
        "for i, prompt in enumerate(user_prompts):\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\":\"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=stream,\n",
        "    )\n",
        "    if stream:\n",
        "        for log in EventLogger().log(response):\n",
        "            log.print()\n",
        "    else:\n",
        "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faa9be3f-3054-4205-b666-e8660f68c306",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(filename='./images/slack-message.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f94501b",
      "metadata": {},
      "source": [
        "In the screenshot above, you can see that our `et-slack-bot`,which is configured to both our public Slack workspace and the Slack MCP server, successfully posted a pod status summary message to the #demos channel."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a41f3a7",
      "metadata": {},
      "source": [
        "### Output Analysis\n",
        "\n",
        "Lets step through the output to further understands whats happening in this notebook.\n",
        "\n",
        "1. First the LLM generated a tool call for the `pods_log` tool included in the **OpenShift MCP server** and fetched the logs for the specified pod.\n",
        "2. The tool successfully retrieved the logs for the pod.\n",
        "3. The LLM  then received the logs from the tool call, along with the original query.\n",
        "4. This context was then passed back to the LLM for the final inference. The inference result provided a summary of the pod logs along with its category of 'Normal' or 'Error'.\n",
        "5. Finally, the summary of the pod logs is sent as a Slack message using the `slack_post_message` tool configured with the **Slack MCP Server**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea60a1b9",
      "metadata": {},
      "source": [
        "## Defining our Agent - ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd7dbe5",
      "metadata": {},
      "source": [
        "Now that we've shown that we can successfully accomplish this multi-step multi-tool task using prompt chaining, let's see if we can give our agent a bit more autonomy to perform the same task but with a single prompt instead of a chain. To do this, we will instantiate a **ReAct agent** (which is included in the llama stack python client by default).The ReAct agent is a variant of the simple agent but with the ability to loop through \"Reason then Act\" iterations, thinking through the problem and then using tools until it determines that it's task has been completed successfully.  \n",
        "\n",
        "Unlike prompt chaining which follows fixed steps, ReAct dynamically breaks down tasks and adapts its approach based on the results of each step. This makes it more flexible and capable of handling complex, real-world queries effectively.\n",
        "\n",
        "Below you will see the slight differences in the agent definition and the prompt used to accomplish our task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d50e9f7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = ReActAgent(\n",
        "            client=client,\n",
        "            model=model_id,\n",
        "            tools=[\"mcp::slack\",\"mcp::openshift\"],\n",
        "            response_format={\n",
        "                \"type\": \"json_schema\",\n",
        "                \"json_schema\": ReActOutput.model_json_schema(),\n",
        "            },\n",
        "            sampling_params={\"max_tokens\":512},\n",
        "        )\n",
        "user_prompts =[\"\"\"Review the OpenShift logs for the pod 'slack-test' in the 'llama-serve' namespace and generate a summary of any errors.\n",
        "               Finally send a clearly formatted message with items pod name, category and explanation to the Slack channel with demo_id `demos`.\"\"\"]\n",
        "session_id = agent.create_session(\"web-session\")\n",
        "for prompt in user_prompts:\n",
        "    print(\"\\n\"+\"=\"*50)\n",
        "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
        "    print(\"=\"*50)\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=stream\n",
        "    )\n",
        "    if stream:\n",
        "        for log in EventLogger().log(response):\n",
        "            log.print()\n",
        "    else:\n",
        "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3998aab7",
      "metadata": {},
      "source": [
        "### Output Analysis\n",
        "\n",
        "Above, we can see that the ReAct agent took nearly an identical approach to the prompt chaining method above, but using a single prompt instead of a chain.  \n",
        "\n",
        "1. First the LLM generated a tool call for the `pods_log` tool included in the **OpenShift MCP server** and fetched the logs for the specified pod.\n",
        "2. The tool successfully retrieved the logs for the pod.\n",
        "3. The LLM  then received the logs from the tool call, along with the original query.\n",
        "4. This context was then passed back to the LLM for the final inference. The inference result provided a summary of the pod logs along with its category of 'Normal' or 'Error'.\n",
        "5. Finally, the summary of the pod logs is sent as a Slack message using the `slack_post_message` tool configured with the **Slack MCP Server**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ecbc8ab-77c6-48ff-970c-2d5dfd54a2c7",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "This notebook demonstrated how to build an agentic MCP applications with Llama Stack. We did this by initializing an agent with access to two MCP servers that were registered to our Llama Stack server, then invoked the agent on our specified set of queries. We showed that we can do this with more directed Prompt Chaining or with the more open ended ReAct pattern. Please check out our next notebooks, [Agents, MCP and RAG](Level6_agents_MCP_and_RAG.ipynb) for more examples of extending your agents capabilities with Llama Stack."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab54e8c7",
      "metadata": {},
      "source": [
        "#### Any Feedback?\n",
        "\n",
        "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
