{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# Level 1: Simple RAG\n",
    "\n",
    "This notebook will show you how to build a simple RAG application with Llama Stack. You will learn how the API's provided by Llama Stack can be used to directly control and invoke all common RAG stages, including indexing, retrieval and inference. \n",
    "\n",
    "_Note: This notebook contains a non-agentic implementation of RAG. We will show you how to build an agentic RAG application later in this tutorial in [Level4_RAG_agent](Level4_RAG_agent.ipynb)._\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Indexing a collection of documents into a vector database for later retrieval.\n",
    "2. Executing the built-in RAG tool to retrieve the document chunks relevant to a given query.\n",
    "3. Using the retrieved context to answer user queries during the inference step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "\n",
    "First, we will start with a few imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from llama_stack_client import RAGDocument\n",
    "from llama_stack_client.types.shared.content_delta import TextDelta, ToolCallDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: granite32-8b\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = \"granite32-8b\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": [
    "Finally, we complete the setup by initializing the document collection we will use for RAG ingestion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": [
    "## 2. Indexing the Documents\n",
    "- Initialize a new document collection in our vector database. All parameters related to the vector database, such as the embedding model and dimension, must be specified here.\n",
    "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, converting, and chunking the content of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    (\"https://www.openshift.guide/openshift-guide-screen.pdf\", \"application/pdf\"),\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## 3. Executing Queries via the Built-in RAG Tool\n",
    "- Directly invoke the RAG tool to query the vector database we ingested into at the previous stage.\n",
    "- Construct an extended prompt using the retrieved chunks.\n",
    "- Query the model with the extended prompt.\n",
    "- Output the reply received from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "User> How do I install OpenShift?\u001b[0m\n",
      "\u001b[35minference> \u001b[0m\u001b[35mTo install OpenShift, follow these steps:\n",
      "\n",
      "1. Open your web browser and navigate to console.redhat.com/openshift/create/local.\n",
      "2. Download the latest release of OpenShift Local and the \"pull secret\" file. The latter is a file containing a key identifying your copy of OpenShift Local to your Red Hat Developer account.\n",
      "3. Unzip the file containing the OpenShift Local executable.\n",
      "4. Using your terminal, run the command `crc setup`. This command will prepare your copy of OpenShift Local, verifying requirements and setting the required configuration values.\n",
      "5. Once the `crc setup` command is ready, launch `crc start`. Running `crc start` can take around 20 minutes on a recent PC.\n",
      "6. Once started, access the OpenShift Web Console with the `crc console` command, which will open your default browser. OpenShift Local uses the developer username and password to log in as a low-privilege user, while the `kubeadmin` user uses a random-generated password. Use the `crc console --credentials` command to find the credentials required to log in as the `kubeadmin` user.\n",
      "\n",
      "Please note that your system must meet certain hardware requirements: a recent Intel CPU (except for Macs, where Apple Silicon machines are supported) with at least four physical cores, at least 16 GB of RAM, and at least 35 GB of free disk space for its installation. The memory requirements may increase in the future, so it's recommended to check the documentation at crc.dev for more up-to-date information.\n",
      "\n",
      "For more detailed information, refer to the official Red Hat OpenShift Local documentation. Even though Red Hat does not officially support other Linux distributions, OpenShift Local can run on others like Ubuntu or Debian with minor caveats. You may need to install additional software packages through your default package manager.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"How do I install OpenShift?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call\n",
    "    rag_response = client.tool_runtime.rag_tool.query(content=prompt, vector_db_ids=[vector_db_id])\n",
    "\n",
    "    # the list of messages to be sent to the model must start with the system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # construct the actual prompt to be executed, incorporating the original query and the retrieved content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # use Llama Stack inference API to directly communicate with the desired model\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,\n",
    "        model_id=model_id,\n",
    "        sampling_params=sampling_params,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    # print the response\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    if stream:\n",
    "        for chunk in response:\n",
    "            response_delta = chunk.event.delta\n",
    "            if isinstance(response_delta, TextDelta):\n",
    "                cprint(response_delta.text, color=\"magenta\", end='')\n",
    "            elif isinstance(response_delta, ToolCallDelta):\n",
    "                cprint(response_delta.tool_call, color=\"magenta\", end='')\n",
    "    else:\n",
    "        cprint(response.completion_message.content, color=\"magenta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "This notebook demonstrated how to set up and use the built-in RAG tool for ingesting user-provided documents in a vector database and utilizing them during inference via direct retrieval. \n",
    "\n",
    "Now that we've seen how easy it is to implement RAG with Llama Stack, We'll move on to building a simple agent with Llama Stack next in our [Simple Agents](./Level2_simple_agent_with_websearch.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5b323",
   "metadata": {},
   "source": [
    "#### Any Feedback?\n",
    "\n",
    "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
