{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# Level 4.5: Agentic RAG with reference Eval\n",
    "\n",
    "This tutorial presents an example of evaluating an agentic RAG in LLama-Stack using the reference implementation. \n",
    "Please refer to `# Level4_agentic_RAG.ipynb` [notebook](../rag_agentic/notebooks/Level4_RAG_agent.ipynb)\n",
    "for details on how to initialize the agent and the knowledge search RAG tool provided by Llama Stack.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Connecting to a llama-stack server.\n",
    "2. Indexing a collection of documents in a vector DB for later retrieval.\n",
    "3. Initializing the agent capable of retrieving content from vector DB via tool use.\n",
    "4. Evaluating the agent responses against a reference set of Q&A.\n",
    "5. Reporting the evaluation results and its statistical relevance.\n",
    "\n",
    "## Case study\n",
    "For the purpose of this training, we are going to use the fictional company \n",
    "[Parasol Financial](https://www.redhat.com/en/blog/ai-insurance-industry-insights-red-hat-summit-2024), and the provided\n",
    "[training documents](https://github.com/jharmison-redhat/parasol-financial-data/).\n",
    "\n",
    "A sample Q&A document is available as a [reference](./data/parasol-financial-data_qac.yaml). \n",
    "This predefined question and answer pairs have beeen generated using [docling-sdg](https://github.com/docling-project/docling-sdg),\n",
    "an IBM set of tools to create artificial data from documents, leveraging generative AI and Docling's parsing capabilities.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have a running instance of the Llama Stack server (local or remote) with at least one preconfigured vector DB. For more information, please refer to the corresponding [Llama Stack tutorials](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html).\n",
    "\n",
    "The `openai` inference provider is required if you intend to use an OpenAI model for judging purposes, like `openai/gpt-4o`. In this case, the \n",
    "`OPENAI_API_KEY` env variable must be configured into the Llama Stack server.\n",
    "\n",
    "## Setting the Environment Variables\n",
    "\n",
    "Use the [`.env.example`](../../../.env.example) to create a new file called `.env` and ensure you add all the relevant environment variables below.\n",
    "\n",
    "In addition to the environment variables listed in the [\"Getting Started\" notebook](../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb), the following should be provided for this demo to run:\n",
    " - `LLM_AS_JUDGE_MODEL_ID`: the model to use as the judge to evaluate the agent responses. Must be one of the models defined in Llama Stack.\n",
    " - `VDB_PROVIDER`: the vector DB provider to be used. Must be supported by Llama Stack. For this demo, we use Milvus Lite which is our preferred solution.\n",
    " - `VDB_EMBEDDING`: the embedding model to be used for ingestion and retrieval. For this demo, we use all-MiniLM-L6-v2.\n",
    " - `VDB_EMBEDDING_DIMENSION` (optional): the dimension of the embedding. Defaults to 384.\n",
    " - `VECTOR_DB_CHUNK_SIZE` (optional): the chunk size for the vector DB. Defaults to 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "We will start with a few imports needed for this demo only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854e7cb4-aed9-4098-adc1-a66f4c9e6ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "from llama_stack_client import Agent, AgentEventLogger, RAGDocument\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab4244-e7af-405b-b0c3-4bf00411f26e",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b87b139-bd18-47b2-889a-1b8ed3018655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server @ http://localhost:8321\n",
      "Inference Parameters:\n",
      "\tModel: granite32-8b\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: True\n",
      "Eval Parameters:\n",
      "\tJudge Model: openai/gpt-4o\n",
      "\tQ&A file: ./data/parasol-financial-data_qac.yaml\n",
      "\tMax rows: 50\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "# to override the judge model\n",
    "from llama_stack.providers.inline.scoring.llm_as_judge.scoring_fn.fn_defs.llm_as_judge_405b_simpleqa import (\n",
    "    llm_as_judge_405b_simpleqa,\n",
    ")\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "remote = os.getenv(\"REMOTE\", \"True\")\n",
    "\n",
    "if remote == \"False\":\n",
    "    local_port = os.getenv(\"LOCAL_SERVER_PORT\", 8321)\n",
    "    base_url = f\"http://localhost:{local_port}\"\n",
    "else: # any value non equal to 'False' will be considered as 'True'\n",
    "    base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=None\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server @ {base_url}\")\n",
    "\n",
    "# model_id will later be used to pass the name of the desired inference model to Llama Stack Agents/Inference APIs\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "# The Q&A file\n",
    "QNA_FILE = './data/parasol-financial-data_qac.yaml'\n",
    "# The number of rows to consider\n",
    "MAX_QNA_ROWS = 50\n",
    "# Set to True to enable display of evaluation results\n",
    "EVAL_DEBUG = False\n",
    "llm_as_judge_model = os.getenv(\"LLM_AS_JUDGE_MODEL_ID\")\n",
    "llm_as_judge_405b_simpleqa_params = llm_as_judge_405b_simpleqa.params.model_copy()\n",
    "# Override the default model\n",
    "# To update the scoring params, we need to provide all the settings, including the defaults\n",
    "llm_as_judge_405b_simpleqa_params.judge_model = llm_as_judge_model\n",
    "\n",
    "# Convert the model dump to a dictionary\n",
    "scoring_params = llm_as_judge_405b_simpleqa_params.model_dump()\n",
    "scoring_params['aggregation_functions']=['categorical_count']\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")\n",
    "print(f\"Eval Parameters:\\n\\tJudge Model: {llm_as_judge_model}\\n\\tQ&A file: {QNA_FILE}\\n\\tMax rows: {MAX_QNA_ROWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357655b5-cade-46f4-9f57-be5dcef9abc2",
   "metadata": {},
   "source": [
    "Finally, we will initialize the document collection to be used for RAG ingestion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583421f3-5c77-4964-b525-12f967c20816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered vector DB **test_vector_db_4ab507b9-8618-4d92-bebf-73c1566578c2**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "display_markdown(f\"Registered vector DB **{vector_db_id}**\", raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203de51-f570-44ab-8130-36333a54888b",
   "metadata": {},
   "source": [
    "## 2. Indexing the Documents\n",
    "- Initialize a new document collection in the target vector DB. All parameters related to the vector DB, such as the embedding model and dimension, must be specified here.\n",
    "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, conversion and chunking of the documents' content.\n",
    "- Perform a sample query to verify the response is retrieved from the relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd4664a-ff7f-4474-b6af-3a4ad3f73052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    \"flexible_enhanced_checking/flexible_enhanced_checking.md\",\n",
    "    \"flexible_savings/flexible_savings.md\",\n",
    "    \"flexible_premier_checking/flexible_premier_checking.md\",\n",
    "    \"flexible_core_checking/flexible_core_checking.md\",\n",
    "    \"policies/online_service_agreement.md\",\n",
    "    \"enablement/customer_interactions_resource_guide.md\",\n",
    "    \"enablement/banking_essentials_resource_guide.md\",\n",
    "    \"flexible_money_market_savings/flexible_money_market_savings.md\",\n",
    "    \"flexible_checking/flexible_checking.md\",\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"{url.split('/')[-1]}\",\n",
    "        content=f\"https://raw.githubusercontent.com/jharmison-redhat/parasol-financial-data/main/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2ee986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flexible_money_market_savings.md',\n",
       " 'flexible_money_market_savings.md',\n",
       " 'flexible_savings.md',\n",
       " 'flexible_savings.md',\n",
       " 'online_service_agreement.md']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query documents\n",
    "results = client.tool_runtime.rag_tool.query(\n",
    "    vector_db_ids=[vector_db_id],\n",
    "    content=\"What is the Parasol Financial Withdrawal Limit Fee and Transaction Limitations for Flexible Money Market Savings\",\n",
    ")\n",
    "results.metadata['document_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99392db2",
   "metadata": {},
   "source": [
    "## 3. Defining reusable functions\n",
    "Define reusable Python functions to use during the execution of the evaluation jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c3b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_categorical_count(response):\n",
    "    \"\"\"\n",
    "    Computes the evaluation accuracy from the responses of the `llm-as-judge::405b-simpleqa`\n",
    "    scoring function.\n",
    "\n",
    "    Expected responses are:\n",
    "    ```\n",
    "    A: CORRECT\n",
    "    B: INCORRECT\n",
    "    C: NOT_ATTEMPTED\n",
    "    ```\n",
    "    The accuracy is computed as: <number of responses of type `A`> / <number of responses> * 100\n",
    "    \"\"\"\n",
    "    # Evaluate numerical score\n",
    "    correct_answers = sum(\n",
    "        [\n",
    "            count\n",
    "            for cat, count in response.scores[\"llm-as-judge::405b-simpleqa\"]\n",
    "            .aggregated_results[\"categorical_count\"][\"categorical_count\"]\n",
    "            .items()\n",
    "            if cat == \"A\"\n",
    "        ]\n",
    "    )\n",
    "    num_of_scores = len(response.scores[\"llm-as-judge::405b-simpleqa\"].score_rows)\n",
    "    return correct_answers / num_of_scores * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb981c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_eval(use_rag: bool):\n",
    "    \"\"\"\n",
    "    Runs the evaluation function for the benchmark indicated by the global variable `qna_benchmark_id`.\n",
    "    A new agent is created for every function call: in case `use_rag` is set to `True`, the `knowledge_search` tool is defined\n",
    "    to implement the RAG workflow.\n",
    "    The global variables `model_id` and `vector_db_id` are also requested.\n",
    "\n",
    "    Params:\n",
    "        use_rag: whether to run a RAG workflow or not.\n",
    "    Returns:\n",
    "        the `Job` associated to the evaluation function.\n",
    "    \"\"\"\n",
    "\n",
    "    from httpx import Timeout\n",
    "\n",
    "    if use_rag == True:\n",
    "        instructions = \"You are a helpful assistant. You must use the knowledge search tool to answer user questions.\"\n",
    "        tools = [\n",
    "            dict(\n",
    "                name=\"builtin::rag\",\n",
    "                args={\n",
    "                    \"vector_db_ids\": [\n",
    "                        vector_db_id\n",
    "                    ],  # list of IDs of document collections to consider during retrieval\n",
    "                },\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        instructions = \"You are a helpful assistant.\"\n",
    "        tools = []\n",
    "\n",
    "    agent_config = {\n",
    "        \"model\": model_id,\n",
    "        \"instructions\": instructions,\n",
    "        \"sampling_params\": sampling_params,\n",
    "        \"toolgroups\": tools,\n",
    "    }\n",
    "\n",
    "    _job = client.eval.run_eval(\n",
    "        benchmark_id=qna_benchmark_id,\n",
    "        benchmark_config={\n",
    "            \"num_examples\": MAX_QNA_ROWS,\n",
    "            \"scoring_params\": {\n",
    "                \"llm-as-judge::405b-simpleqa\": scoring_params,\n",
    "            },\n",
    "            \"eval_candidate\": {\n",
    "                \"type\": \"agent\",\n",
    "                \"config\": agent_config,\n",
    "            },\n",
    "        },\n",
    "        timeout=Timeout(MAX_QNA_ROWS * 30),  # Allow for 30s per Q&A\n",
    "    )\n",
    "    return _job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d406ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_eval_reponse(_job):\n",
    "    \"\"\"\n",
    "    Returns the `EvalResponse` instance for the given `_job`.\n",
    "\n",
    "    Params:\n",
    "        `job_id`: The evaluation `Job`.\n",
    "    Returns:\n",
    "        The `EvalResponse` for the given `_job`\n",
    "    \"\"\"\n",
    "    status = client.eval.jobs.status(\n",
    "        benchmark_id=qna_benchmark_id, job_id=_job.job_id\n",
    "    ).status\n",
    "    while status != \"completed\":\n",
    "        print(f\"Job status is {status}\")\n",
    "        sleep(1)\n",
    "        status = client.eval.jobs.status(\n",
    "            benchmark_id=qna_benchmark_id, job_id=_job.job_id\n",
    "        ).status\n",
    "    print(f\"Job status is {status}\")\n",
    "    _eval_response = client.eval.jobs.retrieve(\n",
    "        benchmark_id=qna_benchmark_id, job_id=_job.job_id\n",
    "    )\n",
    "\n",
    "    return _eval_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b987d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_label(score_row):\n",
    "    \"\"\"\n",
    "    Returns the display label for the given `score_row`.\n",
    "    \"\"\"\n",
    "    grades = {'A': 'CORRECT', 'B': 'INCORRECT', 'C': 'NOT_ATTEMPTED'}\n",
    "    score = score_row.get('score', str(score_row))\n",
    "    return grades.get(score,  f'UNKNOWN {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f3c5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_scores(response):\n",
    "    \"\"\"\n",
    "    Converts the computed scores in a numeric array, where scores `A` are evaluated to 1\n",
    "    and all the others to `0`.\n",
    "    \"\"\"\n",
    "    def category_to_number(category):\n",
    "        if category == 'A':\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    return [category_to_number(score_row['score']) for score_row in response.scores['llm-as-judge::405b-simpleqa'].score_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a5a7f",
   "metadata": {},
   "source": [
    "The next two functions are used to compute and print information about the statistical significance of the results.  This is necessary because you're looking to see if adding RAG truly improves your model, but you only have a limited set of test data (a specific batch of questions). This is a common challenge! Any difference in performance you see could be a real effect of RAG, or it could just be \"luck of the draw\" with that particular sample of questions – this is often called sampling error.  This code helps you distinguish between those possibilities.\n",
    "\n",
    "### What this code does, in a nutshell\n",
    "\n",
    "For each question, the evaluation provider has rated how good the answer was in both with and without RAG conditions. Because you can't test on every possible question, the version with RAG might get higher ratings on your specific set of questions just by chance, even if RAG doesn't offer a consistent, true improvement overall. This code helps you figure out if an observed difference is likely a real impact of RAG or just this \"luck of the draw\" due to the limited test data.\n",
    "\n",
    "It does this by:\n",
    "\n",
    "- Comparing the \"with RAG\" scores against the \"without RAG\" scores.\n",
    "- It runs a statistical check (a \"permutation test\") to see if the difference in average answer ratings on your sample of questions is large enough to suggest RAG has a real effect, or if the difference is likely just due to the random chance of which questions were included in your limited test set.\n",
    "- It then prints a simple summary that includes a p-value to help you assess whether using RAG appears to make a genuine difference (even accounting for the limited data), or there's not enough evidence from your sample to confidently say so.\n",
    "\n",
    "### What is a p-value? (The \"Is the RAG effect real, or just luck from our limited data?\" score)\n",
    "\n",
    "When you only test the model on a limited number of questions, the version with RAG might get higher ratings just by chance on that specific selection of questions. This is the sampling error. The p-value helps you assess this.\n",
    "\n",
    "The p-value tells you: \"If RAG had no real effect on the model's overall ability to answer questions, what's the probability that we'd see a difference in average ratings (between the 'with RAG' and 'without RAG' conditions) as large as (or larger than) the one we found in our limited sample of questions, just due to the random luck of which questions were included in our test?\"\n",
    "\n",
    "- Small p-value (usually less than 0.05): This means \"It's very unlikely we'd see such a difference in our limited sample if RAG actually made no difference overall. The observed improvement (or change) with RAG is probably real and not just a fluke of our specific test questions.\" The code will say the result is \"statistically significant.\"\n",
    "- Large p-value (0.05 or more): This means \"The difference in ratings we saw in our sample (between using RAG and not using RAG) could easily have happened just by chance, given the limited number of questions. We don't have strong enough evidence from this data to say RAG truly improves performance overall.\" The code will say the result is \"NOT statistically significant.\"\n",
    "\n",
    "### How does a permutation test work? (The \"Shuffling to see what 'no RAG effect' looks like with this sample\" Test)\n",
    "\n",
    "The permutation test is a way to see what kind of score differences you might expect from your specific, limited set of questions if RAG actually had no real impact on the model's answers.\n",
    "\n",
    "- Look at the real difference in your sample: First, calculate the actual difference in average answer ratings between the model with RAG and the same model without RAG, based on your limited set of questions.\n",
    "- Shuffle the \"with RAG\" / \"without RAG\" labels within your sample: Now, to simulate the idea that RAG had \"no real effect\" given your actual questions, the test considers each question individually. For each question, you have a pair of ratings: one for the answer with RAG, and one for the answer without RAG. The test randomly swaps which rating is labeled \"with RAG\" and which is labeled \"without RAG\" for that question. This is done for all questions in your sample, and this whole shuffling process is repeated many times (e.g., 10,000 times).\n",
    "- Calculate \"luck-based\" differences from your sample: Each time it shuffles these labels, it calculates a new \"fake\" overall difference in average ratings between the (now jumbled) \"with RAG\" condition and the \"without RAG\" condition. This creates a collection of differences that could have occurred with your specific set of questions purely by chance, if RAG truly offered no consistent advantage or disadvantage.\n",
    "- Compare: Finally, it compares your actual real difference (from step 1, on your limited sample) to all these \"fake,\" luck-based differences (generated from shuffling within your sample).\n",
    "    - If your actual difference (e.g., the improvement seen with RAG) is quite extreme compared to the shuffled ones, it means it's unlikely to be just a result of which particular questions ended up in your limited test set. This suggests RAG has a real effect, leading to a small p-value.\n",
    "    - If your actual difference looks typical among the shuffled ones, it means it could easily be explained by the random variation inherent in your limited sample of questions if RAG had no true effect. This leads to a large p-value.\n",
    "\n",
    "### How do we estimate the margin of error?\n",
    "\n",
    "When the results are not statistically significant, we provide a rough heuristic-based \"margin of error.\" The conclusion to draw from a statistically insignificant result is that we don't have enough evidence to say the two conditions are different. Therefore, they might be roughly equivalent, or one might be slightly better but our test didn't have enough power (often due to sample size) to detect it. The rough \"margin of error\" can give you a basic sense of how large a difference could be hiding due to this uncertainty.\n",
    "\n",
    "It computes the margin of error as plus or minus `1/sqrt(num_samples)`, which is a very popular heuristic for estimating the margin of error for a proportion from a sample. For example, if you have 256 questions in your test set, it computes a margin of error of `1/sqrt(256)`, which is one-sixteenth, i.e., +/- 6.25%. This suggests that with 256 questions, if you're looking at a percentage, your measured value is likely to be within 6.25% or so of the true value you'd get with many more questions. When comparing two conditions, it means that if the real difference between them is smaller than this +/- 6.25%, your test might not be able to reliably detect it.\n",
    "\n",
    "You can think of this like a ruler with markers for each sixteenth of an inch. You can use it to measure lengths to within about a sixteenth of an inch or so. If you're trying to see if one object is longer than another, and they both appear to be the same length on this ruler, they could still actually be different by an amount smaller than a sixteenth of an inch – your ruler just isn't precise enough to tell. Similarly, if our statistical test doesn't find a significant difference between two conditions, the 'margin of error' gives you a sense of how large a real difference might be while still being 'hidden' by the limitations of our sample size (our 'ruler's' precision). If you wanted a more exact measurement of length (or a more certain detection of a difference), you would need a ruler with finer markings (analogous to needing more test questions).\n",
    "\n",
    "### Summary\n",
    "\n",
    "In essence, the code helps you make more informed judgments about whether RAG genuinely impacts your model's performance, by trying to separate a true effect from the \"noise\" or \"luck\" that can arise from working with a finite amount of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a375726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test_for_paired_samples(scores_a, scores_b, iterations=10_000):\n",
    "    \"\"\"\n",
    "    Performs a permutation test of a given statistic on provided data.\n",
    "    \"\"\"\n",
    "\n",
    "    from scipy.stats import permutation_test\n",
    "\n",
    "\n",
    "    def _statistic(x, y, axis):\n",
    "        return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
    "\n",
    "    result = permutation_test(\n",
    "        data=(scores_a, scores_b),\n",
    "        statistic=_statistic,\n",
    "        n_resamples=iterations,\n",
    "        alternative='two-sided',\n",
    "        permutation_type='samples'\n",
    "    )\n",
    "    return float(result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee78f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats_significance(scores_a, scores_b, label_a, label_b):\n",
    "    mean_score_a = np.mean(scores_a)\n",
    "    mean_score_b = np.mean(scores_b)\n",
    "\n",
    "    p_value = permutation_test_for_paired_samples(scores_a, scores_b)\n",
    "    print(model_id)\n",
    "    print(f\" {label_a:<50}: {mean_score_a:>10.4f}\")\n",
    "    print(f\" {label_b:<50}: {mean_score_b:>10.4f}\")\n",
    "    print(f\" {'p_value':<50}: {p_value:>10.4f}\")\n",
    "    print()\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"p_value<0.05 so this result is statistically significant\")\n",
    "        # Note that the logic below if wrong if the mean scores are equal, but that can't be true if p<1.\n",
    "        higher_model_id = (\n",
    "            label_a\n",
    "            if mean_score_a >= mean_score_b\n",
    "            else label_b\n",
    "        )\n",
    "        print(f\"You can conclude that {higher_model_id} generation is better on data of this sort\")\n",
    "    else:\n",
    "        import math\n",
    "\n",
    "        print(\"p_value>=0.05 so this result is NOT statistically significant.\")\n",
    "        print(\n",
    "            f\"You can conclude that there is not enough data to tell which is better.\"\n",
    "        )\n",
    "        num_samples = len(scores_a)\n",
    "        margin_of_error = 1 / math.sqrt(num_samples)\n",
    "        print(\n",
    "            f\"Note that this data includes {num_samples} questions which typically produces a margin of error of around +/-{margin_of_error:.1%}.\"\n",
    "        )\n",
    "        print(f\"So the two are probably roughly within that margin of error or so.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84253d",
   "metadata": {},
   "source": [
    "## 4. Creating an evaluation Dataset\n",
    "- Load the Q&A file as a Pandas DataFrame.\n",
    "- Transform the dataset to a schema suitable for LLS evaluations.\n",
    "- Register a new Dataset.\n",
    "- Register a Benchmark using the Dataset and the `llm-as-judge::405b-simpleqa` scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca041b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(QNA_FILE, \"r\") as f:\n",
    "    qnas_df = pd.read_json(f, lines=True)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "145d5514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_query</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>chat_completion_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the phone number provided for contacting Parasol Financial?</td>\n",
       "      <td>800.867.5309</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What is the phone number provided for contacting Parasol Financial?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why might the company include exceptions to their liability in the agreement?</td>\n",
       "      <td>The company includes exceptions to their liability to protect themselves from situations that are beyond their control or not their fault, such as insufficient funds in the customer's account, known service malfunctions, uncontrollable events like natural disasters, and delays caused by third parties.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"Why might the company include exceptions to their liability in the agreement?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the main purposes for which the Bank shares anonymized transaction information?</td>\n",
       "      <td>The Bank shares anonymized transaction information to facilitate participation in the rewards program, present offers of interest, and administer benefits and rewards with participating merchants, third parties, and card networks.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What are the main purposes for which the Bank shares anonymized transaction information?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What happens if the person to whom you are sending money does not enroll with Zelle within 14 days?</td>\n",
       "      <td>The transfer will be canceled.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What happens if the person to whom you are sending money does not enroll with Zelle within 14 days?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What might be a consequence of opting out of security alerts?</td>\n",
       "      <td>A consequence of opting out of security alerts might be that the user will not receive any notifications about potential security issues with their credit card, business line of credit, or debit card, which could leave them unaware of unauthorized transactions or other security concerns.</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"What might be a consequence of opting out of security alerts?\", \"context\": null}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           input_query  \\\n",
       "0                                  What is the phone number provided for contacting Parasol Financial?   \n",
       "1                        Why might the company include exceptions to their liability in the agreement?   \n",
       "2             What are the main purposes for which the Bank shares anonymized transaction information?   \n",
       "3  What happens if the person to whom you are sending money does not enroll with Zelle within 14 days?   \n",
       "4                                        What might be a consequence of opting out of security alerts?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                  expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                    800.867.5309   \n",
       "1  The company includes exceptions to their liability to protect themselves from situations that are beyond their control or not their fault, such as insufficient funds in the customer's account, known service malfunctions, uncontrollable events like natural disasters, and delays caused by third parties.   \n",
       "2                                                                          The Bank shares anonymized transaction information to facilitate participation in the rewards program, present offers of interest, and administer benefits and rewards with participating merchants, third parties, and card networks.   \n",
       "3                                                                                                                                                                                                                                                                                  The transfer will be canceled.   \n",
       "4                A consequence of opting out of security alerts might be that the user will not receive any notifications about potential security issues with their credit card, business line of credit, or debit card, which could leave them unaware of unauthorized transactions or other security concerns.   \n",
       "\n",
       "                                                                                                                                   chat_completion_input  \n",
       "0                                  [{\"role\": \"user\", \"content\": \"What is the phone number provided for contacting Parasol Financial?\", \"context\": null}]  \n",
       "1                        [{\"role\": \"user\", \"content\": \"Why might the company include exceptions to their liability in the agreement?\", \"context\": null}]  \n",
       "2             [{\"role\": \"user\", \"content\": \"What are the main purposes for which the Bank shares anonymized transaction information?\", \"context\": null}]  \n",
       "3  [{\"role\": \"user\", \"content\": \"What happens if the person to whom you are sending money does not enroll with Zelle within 14 days?\", \"context\": null}]  \n",
       "4                                        [{\"role\": \"user\", \"content\": \"What might be a consequence of opting out of security alerts?\", \"context\": null}]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_stack.apis.inference import UserMessage\n",
    "import json\n",
    "import random\n",
    "\n",
    "qna_dataset_rows = []\n",
    "\n",
    "chat_completion_input = UserMessage(content=\"\")\n",
    "for i in range(len(qnas_df)):\n",
    "    qna = {}\n",
    "    qna[\"input_query\"] = qnas_df.iloc[i][\"question\"]\n",
    "    qna[\"expected_answer\"] = qnas_df.iloc[i][\"answer\"]\n",
    "\n",
    "    chat_completion_input.content = qna[\"input_query\"]\n",
    "    qna[\"chat_completion_input\"] = json.dumps([chat_completion_input.model_dump()])\n",
    "\n",
    "    qna_dataset_rows.append(qna)\n",
    "\n",
    "random.shuffle(qna_dataset_rows)\n",
    "qna_dataset_df = pd.DataFrame(qna_dataset_rows)\n",
    "qna_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22fd5734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered dataset **test_dataset_0eea3690-29a7-11f0-a45b-4a70c355aff9**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qna_dataset_id = f\"test_dataset_{uuid.uuid1()}\"\n",
    "_ = client.datasets.register(\n",
    "    purpose=\"eval/messages-answer\",\n",
    "    source={\n",
    "        \"type\": \"rows\",\n",
    "        \"rows\": qna_dataset_rows,\n",
    "    },\n",
    "    dataset_id=qna_dataset_id,\n",
    ")\n",
    "display_markdown(f\"Registered dataset **{qna_dataset_id}**\", raw=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b669a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered benchmark **test_benchmark_0f4b9f52-29a7-11f0-a45b-4a70c355aff9**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qna_benchmark_id = f\"test_benchmark_{uuid.uuid1()}\"\n",
    "client.benchmarks.register(\n",
    "    benchmark_id=qna_benchmark_id,\n",
    "    dataset_id=qna_dataset_id,\n",
    "    scoring_functions=[\"llm-as-judge::405b-simpleqa\"],\n",
    ")\n",
    "display_markdown(f\"Registered benchmark **{qna_benchmark_id}**\", raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd43d5",
   "metadata": {},
   "source": [
    "## 5. LLM Eval without RAG\n",
    "- Create an agent configuration without the `knowledge_search` tool.\n",
    "- Run the evaluation function with the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "451b9177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status is completed\n",
      "Evaluation of 50 Q&A workflows completed in 387.319 seconds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Computed accuracy is 40.0%**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "without_rag_responses = {}\n",
    "_job = _run_eval(use_rag=False)\n",
    "# pprint(_job)\n",
    "_eval_response = _get_eval_reponse(_job)\n",
    "if EVAL_DEBUG == True:\n",
    "    pprint(_eval_response)\n",
    "\n",
    "print(\n",
    "    f\"Evaluation of {MAX_QNA_ROWS} Q&A workflows completed in {time.time() - start:.3f} seconds\"\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**Computed accuracy is {accuracy_from_categorical_count(_eval_response)}%**\",\n",
    "    raw=True,\n",
    ")\n",
    "without_rag_responses[model_id] = _eval_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe7cd9",
   "metadata": {},
   "source": [
    "## 6. LLM Eval with RAG\n",
    "- Create an agent configuration with the `knowledge_search` tool.\n",
    "- Run the evaluation function with the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32b7402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status is completed\n",
      "Evaluation of 50 Q&A workflows completed in 264.498 seconds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Computed accuracy is 64.0%**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**RAG knowledge search tool used in 48 of (50) agentic calls**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "rag_responses = {}\n",
    "_job = _run_eval(use_rag=True)\n",
    "# pprint(_job)\n",
    "_eval_response = _get_eval_reponse(_job)\n",
    "if EVAL_DEBUG == True:\n",
    "    pprint(_eval_response)\n",
    "\n",
    "print(\n",
    "    f\"Evaluation of {MAX_QNA_ROWS} Q&A workflows completed in {time.time() - start:.3f} seconds\"\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**Computed accuracy is {accuracy_from_categorical_count(_eval_response)}%**\",\n",
    "    raw=True,\n",
    ")\n",
    "rag_responses[model_id] = _eval_response\n",
    "\n",
    "retrieved_contexts = sum(\n",
    "    [1 for r in rag_responses[model_id].generations if \"context\" in r]\n",
    ")\n",
    "display_markdown(\n",
    "    f\"**RAG knowledge search tool used in {retrieved_contexts} of ({MAX_QNA_ROWS}) agentic calls**\",\n",
    "    raw=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## 7. Reporting\n",
    "- Aggregated accuracy.\n",
    "- Individual scores and responses.\n",
    "- Statistical Significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4df0d8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3e9a3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_3e9a3_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_3e9a3_level0_col1\" class=\"col_heading level0 col1\" >Accuracy without RAG</th>\n",
       "      <th id=\"T_3e9a3_level0_col2\" class=\"col_heading level0 col2\" >Accuracy with RAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_3e9a3_row0_col0\" class=\"data row0 col0\" >granite32-8b</td>\n",
       "      <td id=\"T_3e9a3_row0_col1\" class=\"data row0 col1\" >40.000000</td>\n",
       "      <td id=\"T_3e9a3_row0_col2\" class=\"data row0 col2\" >64.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10f6cbb60>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_responses = {}\n",
    "pd_responses['questions'] = [qna_dataset_rows[i]['input_query'] for i in range(MAX_QNA_ROWS)]\n",
    "pd_responses['expected'] = [qna_dataset_rows[i]['expected_answer'] for i in range(MAX_QNA_ROWS)]\n",
    "\n",
    "pd_accuracies = {}\n",
    "df_accuracies = pd.DataFrame.from_dict({\n",
    "    'Model': without_rag_responses.keys(),\n",
    "    'Accuracy without RAG': [accuracy_from_categorical_count(without_rag_responses[model_id]) for model_id in without_rag_responses.keys()],\n",
    "    'Accuracy with RAG': [accuracy_from_categorical_count(rag_responses[model_id]) for model_id in rag_responses.keys()]})\n",
    "df_accuracies.style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95b9baa2-4739-426a-b79a-2ff90f44c023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report_data = {}\n",
    "ratings_data = {}\n",
    "responses_data = {}\n",
    "\n",
    "report_data['Question'] = [qna_dataset_rows[i]['input_query'] for i in range(MAX_QNA_ROWS)]\n",
    "ratings_data['Question'] = report_data['Question']\n",
    "responses_data['Question'] = report_data['Question']\n",
    "report_data['Expected Answer'] = [qna_dataset_rows[i]['expected_answer'] for i in range(MAX_QNA_ROWS)]\n",
    "responses_data['Expected Answer'] = report_data['Expected Answer']\n",
    "for model_id in without_rag_responses.keys():\n",
    "    report_data[f'Rating without RAG'] = [to_label(score_row) for score_row in without_rag_responses[model_id].scores['llm-as-judge::405b-simpleqa'].score_rows]\n",
    "    report_data[f'Answer without RAG'] = [g['generated_answer'] for g in without_rag_responses[model_id].generations]\n",
    "    report_data[f'Rating with RAG'] = [to_label(score_row) for score_row in rag_responses[model_id].scores['llm-as-judge::405b-simpleqa'].score_rows]\n",
    "    report_data[f'Answer with RAG'] = [g['generated_answer'] for g in rag_responses[model_id].generations]\n",
    "    \n",
    "    ratings_data[f'Rating without RAG'] = report_data[f'Rating without RAG']\n",
    "    responses_data[f'Answer without RAG'] = report_data[f'Answer without RAG']\n",
    "    ratings_data[f'with RAG RAG Rating'] = report_data[f'Rating with RAG']\n",
    "    responses_data[f'Answer with RAG'] = report_data[f'Answer with RAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "272729b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response Comparison for model granite32-8b**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Question</th>\n",
       "      <th>Expected Answer</th>\n",
       "      <th>Rating without RAG</th>\n",
       "      <th>Answer without RAG</th>\n",
       "      <th>Rating with RAG</th>\n",
       "      <th>Answer with RAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>What is the phone number provided for contacting Parasol Financial?</td>\n",
       "      <td>800.867.5309</td>\n",
       "      <td>NOT_ATTEMPTED</td>\n",
       "      <td>I'm sorry for any inconvenience, but as an AI, I don't have real-time access to specific databases or the ability to browse the internet. Therefore, I can't provide the current contact number for Parasol Financial. I recommend visiting their official website or doing a quick online search to find the most accurate and up-to-date contact information.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>The phone number provided for contacting Parasol Financial is 800.867.5309.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Why might the company include exceptions to their liability in the agreement?</td>\n",
       "      <td>The company includes exceptions to their liability to protect themselves from situations that are beyond their control or not their fault, such as insufficient funds in the customer's account, known service malfunctions, uncontrollable events like natural disasters, and delays caused by third parties.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Companies often include exceptions to their liability in agreements for several reasons:\\n\\n1. **Risk Management**: Every business operation involves some level of risk. By specifying exceptions, companies can clearly outline the circumstances under which they won't be held responsible. This helps in managing and mitigating potential risks.\\n\\n2. **Legal Protection**: These exceptions serve as a legal safeguard. They help protect the company from unforeseen or unusual circumstances that might lead to claims or lawsuits. \\n\\n3. **Clarity and Transparency**: Exceptions clearly define the boundaries of the company's responsibilities. This transparency can help prevent misunderstandings or disputes down the line.\\n\\n4. **Standard Industry Practice**: In many industries, it's common to include such clauses in contracts. They're often part of standard form agreements, and their inclusion can be seen as a standard business practice.\\n\\n5. **Cost Control**: By limiting their liability, companies can control their potential financial exposure. This can be particularly important for smaller businesses or startups.\\n\\n6. **Force Majeure**: This is a specific type of exception that absolves a company from liability due to unforeseeable circumstances beyond their control, such as natural disasters, wars, or strikes.\\n\\n7. **Intellectual Property**: In agreements involving intellectual property, exceptions might be included to protect the company's own IP or to limit liability related to third-party IP.\\n\\nRemember, the specific reasons can vary based on the nature of the business, the type of agreement, and the jurisdiction in which it's being conducted. It's always important to have legal advice when interpreting or negotiating such clauses.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>The company might include exceptions to their liability in the agreement for several reasons:\\n\\n1. **Risk Management**: By specifying exceptions, the company can clearly outline the circumstances under which they won't be held responsible. This helps in managing potential risks and liabilities.\\n\\n2. **Legal Protection**: Exceptions can provide legal protection for the company. They can limit the company's liability in situations that are beyond their control or due to circumstances that were disclosed upfront.\\n\\n3. **Clarity and Transparency**: Exceptions clearly define the boundaries of the company's responsibilities. This transparency can help in setting clear expectations for both parties involved in the agreement.\\n\\n4. **Fairness**: In some cases, certain risks or damages might be inherent to the service or product being provided. Excluding liability for these would be fair, as the company can't reasonably be expected to cover these.\\n\\n5. **Cost Control**: By limiting liability, companies can control their potential financial exposure. This can help in managing costs and maintaining profitability.\\n\\nThese are general reasons and the specific rationale would depend on the nature of the agreement and the company's business. To get a more precise answer, you could use the knowledge_search function with details about the specific agreement or company in question.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What are the main purposes for which the Bank shares anonymized transaction information?</td>\n",
       "      <td>The Bank shares anonymized transaction information to facilitate participation in the rewards program, present offers of interest, and administer benefits and rewards with participating merchants, third parties, and card networks.</td>\n",
       "      <td>NOT_ATTEMPTED</td>\n",
       "      <td>Banks share anonymized transaction information primarily for several key purposes:\\n\\n1. **Fraud Detection and Prevention**: By analyzing patterns and trends in transaction data, banks can identify unusual activities that may indicate fraudulent behavior. This helps in protecting both the bank and its customers from financial losses.\\n\\n2. **Risk Management**: Anonymized data can be used to assess and manage various types of risk, such as credit risk, market risk, and operational risk. This helps banks to make informed decisions about lending, investment, and operational strategies.\\n\\n3. **Regulatory Compliance**: Banks are often required by law to share certain types of transaction data with regulatory bodies. This is to ensure transparency, prevent money laundering, and maintain financial stability.\\n\\n4. **Product Development and Marketing**: Anonymized data can provide insights into customer behavior and preferences, which can guide the development of new products and services, and inform marketing strategies.\\n\\n5. **Research and Analytics**: Banks and financial institutions often collaborate with academic institutions or use their own research teams to analyze anonymized data. This can lead to new insights about economic trends, consumer behavior, and financial systems.\\n\\n6. **Cybersecurity**: By studying patterns of normal and abnormal transactions, banks can improve their cybersecurity measures to protect against cyber threats and attacks.\\n\\nIt's important to note that in all these cases, the data is anonymized to protect the privacy of individual customers. Personal identifiers are removed or encrypted to ensure that the shared information cannot be traced back to specific individuals.</td>\n",
       "      <td>INCORRECT</td>\n",
       "      <td>The main purposes for which a bank might share anonymized transaction information are primarily related to security and fraud prevention. This can include:\\n\\n1. **Fraud Detection and Prevention**: Banks use anonymized transaction data to identify unusual patterns or activities that could indicate fraudulent behavior, such as unauthorized payments or scams.\\n\\n2. **Money Laundering Prevention**: Anonymized data can help banks detect and prevent money laundering activities by identifying suspicious transactions.\\n\\n3. **Digital Currency Monitoring**: Banks may share anonymized transaction data to monitor digital currencies like bitcoins, which are often associated with high-risk transactions.\\n\\n4. **System Improvement**: Analyzing anonymized transaction data can help banks understand how their systems are being used, identify potential issues, and improve their services.\\n\\n5. **Regulatory Compliance**: Banks are often required by law to share certain types of transaction data with regulatory bodies to ensure compliance with financial regulations.\\n\\nIt's important to note that any sharing of transaction data is done in an anonymized form to protect customer privacy. The specific purposes can vary depending on the bank's policies and the regulations in their jurisdiction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What happens if the person to whom you are sending money does not enroll with Zelle within 14 days?</td>\n",
       "      <td>The transfer will be canceled.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>If the person you're sending money to doesn't enroll with Zelle within 14 days, the transaction may be reversed, and the funds could be returned to your account. This is because Zelle requires the recipient to enroll and accept the money within a certain timeframe to ensure the transaction's legitimacy and security. \\n\\nHowever, the exact policy can vary depending on the bank or financial institution you're using. Some banks might have different timeframes or policies for handling such situations. It's always a good idea to check with your bank for their specific Zelle policy. \\n\\nAlso, keep in mind that once the money is returned to your account, it might take a few business days for it to reflect in your account balance, depending on your bank's processing times. \\n\\nTo avoid such situations, it's recommended to confirm with the recipient that they've enrolled in Zelle and are expecting the funds before initiating the transfer.</td>\n",
       "      <td>INCORRECT</td>\n",
       "      <td>If the person to whom you are sending money via Zelle does not enroll within 14 days, the transfer will be canceled. The money may also be delayed or the transfer may be blocked to prevent fraud or comply with regulatory requirements.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What might be a consequence of opting out of security alerts?</td>\n",
       "      <td>A consequence of opting out of security alerts might be that the user will not receive any notifications about potential security issues with their credit card, business line of credit, or debit card, which could leave them unaware of unauthorized transactions or other security concerns.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Opting out of security alerts can have several potential consequences:\\n\\n1. **Increased Vulnerability**: Security alerts often notify users about potential threats, vulnerabilities, or suspicious activities. By opting out, you might miss crucial information that could help protect your system or data from cyber threats.\\n\\n2. **Delayed Response to Threats**: Without alerts, you might not be immediately aware of a security breach or threat. This delay could allow the threat to escalate or cause more damage before it's detected and addressed.\\n\\n3. **Non-compliance**: In some cases, especially in corporate or regulatory environments, opting out of security alerts could lead to non-compliance with security policies or legal requirements. This could result in penalties or sanctions.\\n\\n4. **Missed Updates**: Security alerts often accompany updates or patches that fix vulnerabilities. By not receiving these alerts, you might miss out on important updates, leaving your system exposed.\\n\\n5. **Reduced Awareness**: Regular exposure to security alerts can increase your general awareness about cybersecurity. Opting out could reduce this awareness, making you less cautious and more susceptible to phishing attempts, malware, or other cyber threats.\\n\\n6. **Potential for Unnoticed Data Breaches**: If a breach occurs and you're not receiving alerts, you might not realize that your data has been compromised, which could lead to identity theft, financial loss, or other negative consequences.\\n\\nRemember, the specific consequences can vary depending on the context (individual user, corporate environment, etc.) and the nature of the security alerts in question.</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Opting out of security alerts may leave you uninformed about potential security threats or suspicious activities related to your account. This could potentially expose you to risks such as unauthorized access, fraudulent transactions, or identity theft. It's crucial to stay updated with security-related information to protect your financial interests effectively.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "display_markdown(f\"**Response Comparison for model {model_id}**\", raw=True)\n",
    "report_df = pd.DataFrame.from_dict(report_data)\n",
    "HTML(report_df.head().to_html(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5409d599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Statistical Significance (without Vs with RAG generations)**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "granite32-8b\n",
      " accuracy without RAG                              :     0.4000\n",
      " accuracy with RAG                                 :     0.6400\n",
      " p_value                                           :     0.0040\n",
      "\n",
      "p_value<0.05 so this result is statistically significant\n",
      "You can conclude that accuracy with RAG generation is better on data of this sort\n"
     ]
    }
   ],
   "source": [
    "display_markdown(\"**Statistical Significance (without Vs with RAG generations)**\", raw= True)\n",
    "print_stats_significance(numeric_scores(without_rag_responses[model_id]), numeric_scores(rag_responses[model_id]), \"accuracy without RAG\", \"accuracy with RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "This tutorial demonstrates how to evaluate an agentic workflow, without and without RAG tool, using the Llama Stack reference implementation.\n",
    "We do so by initializing an agent, with optional access to the RAG tool, then invoking the agent evaluation against a predefined reference of sample Q&A. \n",
    "Please check out our [complementary tutorial](../rag_agentic/notebooks/Level4_RAG_agent.ipynb) for an agentic RAG example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
