# This job takes ~20min to complete.
# This will create a very large yaml file. To extract the file, run:
# oc apply -f retriever-job.yaml
# oc apply -f accessor-pod.yaml
# mkdir ./guidellm-reports
# oc rsync guidellm-accessor:/mnt/output/guidellm-reports.tgz ./guidellm-reports
# You will now have a local ./guidellm-reports/guidellm-reports.tgz, to extract it run:
# tar -xvf guidellm-reports.tgz
# You will now have a local file ./guidellm-reports/llama32-3b.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: run-guidellm
spec:
  template:
    spec:
      containers:
      - name: guidellm
        # TODO: replace this image
        image: quay.io/sallyom/guidellm:entrypoint
        imagePullPolicy: Always
        args:
        - --target=$(TARGET)
        - --data-type=$(DATA_TYPE)
        - --data=$(DATA)
        - --output-path=/output/llama32-3b.yaml
        env:
        # HF_TOKEN is not necessary if you share/use the model PVC. Guidellm needs to access the tokenizer file.
        # You can provide a path to the tokenizer file by passing `--tokenizer=/path/to/model`. If you do not
        # pass the tokenizer path, Guidellm will get the tokenizer file(s) from Huggingface.
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              key: HF_TOKEN
              name: huggingface-secret
        - name: TARGET
          value: "http://llama32-3b.llama-serve.svc.cluster.local:8000/v1"
        - name: DATA_TYPE
          value: "emulated"
        - name: DATA
          value: "prompt_tokens=512,generated_tokens=128"
        volumeMounts:
        - name: output
          mountPath: /output
      restartPolicy: Never
      volumes:
      - name: output
        persistentVolumeClaim:
          claimName: guidellm-output-pvc
  backoffLimit: 0
